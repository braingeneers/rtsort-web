{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce503323",
   "metadata": {},
   "source": [
    "# Torch vs. ONNX\n",
    "\n",
    "Verify that the spike detection output of torch vs. onnx in python is concordant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8f40de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7293e7",
   "metadata": {},
   "source": [
    "## Load Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b0240c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a trained model\n",
    "from braindance.core.spikedetector.model import ModelSpikeSorter\n",
    "\n",
    "# detection_model = ModelSpikeSorter.load(\"checkpoints/spikedetector/mea\")\n",
    "\n",
    "with open(\"checkpoints/spikedetector/mea/init_dict.json\", \"r\") as f:\n",
    "    init_dict = json.load(f)\n",
    "pytorch_model = ModelSpikeSorter(**init_dict)\n",
    "state_dict = torch.load(\n",
    "    \"checkpoints/spikedetector/mea/state_dict.pt\", map_location=\"cpu\"\n",
    ")\n",
    "pytorch_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09447e",
   "metadata": {},
   "source": [
    "## Run via Braindance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a958efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping saving scaled traces because file scaled_traces.npy already exists and debug=True\n",
      "Skipping running detection model because file model_outputs.npy already exists and debug=True\n",
      "Skipping detecting preliminary propagation sequences because file all_clusters.pickle already exists and debug=True\n",
      "Skipping reassigning spikes because file all_clusters_reassigned.pickle already exists and debug=True\n",
      "Merging preliminary propagation sequences - first round\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 sequences after first merging\n",
      "Merging preliminary propagation sequences - second round ...\n",
      "\n",
      "RT-Sort detected 7 sequences\n"
     ]
    }
   ],
   "source": [
    "from braindance.core.spikesorter.rt_sort import detect_sequences\n",
    "\n",
    "# Detect sequences in the first 5 minutes of a recording\n",
    "rt_sort = detect_sequences(\n",
    "    \"data/MEA_rec_patch_ground_truth_cell7.raw.h5\",\n",
    "    \"data/inter\",\n",
    "    pytorch_model,\n",
    "    recording_window_ms=(0, 5 * 1000),\n",
    "    device=\"cpu\",\n",
    "    verbose=True,\n",
    "    debug=True,\n",
    "    # num_processes=1,  # Uncomment for debugging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190da61",
   "metadata": {},
   "source": [
    "Output with debug=False\n",
    "```\n",
    "Saving traces:\n",
    "100%|██████████| 1/1 [00:04<00:00,  4.46s/it]\n",
    "Running detection model:\n",
    "Compiling detection model for 942 elecs ...\n",
    "Cannot compile detection model with torch_tensorrt because cannot load torch_tensorrt. Skipping NVIDIA compilation\n",
    "Allocating disk space to save model traces and outputs ...\n",
    "Inference scaling: 0.3761194029850746\n",
    "Running model ...\n",
    "100%|██████████| 832/832 [09:36<00:00,  1.44it/s]\n",
    "Detecting sequences\n",
    "100%|██████████| 942/942 [00:04<00:00, 211.68it/s]\n",
    "Detected 10 preliminary propagation sequences\n",
    "Extracting sequences' detections, intervals, and amplitudes\n",
    "\n",
    "100%|██████████| 10/10 [00:02<00:00,  4.03it/s]\n",
    "8 clusters remain after filtering\n",
    "Reassigning spikes to preliminary propagation sequences\n",
    "Initializing ...\n",
    "Sorting recording\n",
    "100%|██████████| 1000/1000 [00:00<00:00, 3377.24it/s]\n",
    "Extracting sequences' detections, intervals, and amplitudes\n",
    "\n",
    "100%|██████████| 7/7 [00:02<00:00,  2.80it/s]\n",
    "7 clusters remain after filtering\n",
    "Merging preliminary propagation sequences - first round\n",
    "\n",
    "100%|██████████| 7/7 [00:02<00:00,  3.14it/s]\n",
    "7 sequences after first merging\n",
    "Merging preliminary propagation sequences - second round ...\n",
    "\n",
    "RT-Sort detected 7 sequences\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447b7c1",
   "metadata": {},
   "source": [
    "## Run via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2368c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_scaling: 0.3761194029850746\n"
     ]
    }
   ],
   "source": [
    "# Calculate inference scaling factor based on the median IQR of the first 1000 frames\n",
    "import scipy.stats\n",
    "\n",
    "scaled_traces = np.load(\"data/inter/scaled_traces.npy\")\n",
    "\n",
    "# Defaults from rt_sort.py\n",
    "inference_scaling_numerator = 12.6\n",
    "pre_median_frames = 1000\n",
    "\n",
    "window = scaled_traces[:, :pre_median_frames]\n",
    "iqrs = scipy.stats.iqr(window, axis=1)\n",
    "median = np.median(iqrs)\n",
    "inference_scaling = inference_scaling_numerator / median\n",
    "print(\"inference_scaling:\", inference_scaling)\n",
    "\n",
    "# From output of detect_sequences call above should be 0.3761194029850746\n",
    "assert np.isclose(inference_scaling, 0.3761194029850746, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95696ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a single input frame to the model\n",
    "traces_torch = torch.tensor(\n",
    "    scaled_traces[:, 0 : pytorch_model.sample_size],\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "traces_torch -= torch.median(traces_torch, dim=1, keepdim=True).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef175ac4",
   "metadata": {},
   "source": [
    "## Compare PyTorch to Braindance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8dda973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model inputs are concordant\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit comparison to the first num_output_locs locations as there is some overlap\n",
    "model_traces = np.load(\"data/inter/model_traces.npy\")\n",
    "print(\"Model inputs are concordant\")\n",
    "np.isclose(\n",
    "    traces_torch.numpy()[:, 0 : pytorch_model.num_output_locs],\n",
    "    model_traces[:, 0 : pytorch_model.num_output_locs],\n",
    "    rtol=1e-5,\n",
    ").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6c7723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model outputs are concordant\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs = pytorch_model.model(model_input[0:1, :, 0:100]).cpu()\n",
    "pytorch_outputs = pytorch_model.model(\n",
    "    traces_torch[:, None, :] * pytorch_model.input_scale * inference_scaling\n",
    ")\n",
    "model_outputs = np.load(\"data/inter/model_outputs.npy\")\n",
    "print(\"Model outputs are concordant\")\n",
    "np.isclose(\n",
    "    pytorch_outputs.detach().numpy()[:, 0 : pytorch_model.num_output_locs],\n",
    "    model_outputs[:, 0 : pytorch_model.num_output_locs],\n",
    "    rtol=1e-5,\n",
    ").all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ffffb",
   "metadata": {},
   "source": [
    "# Export to .onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7913d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# num_channels = init_dict[\"num_channels_in\"]\n",
    "# sample_size = init_dict[\"sample_size\"]\n",
    "\n",
    "# model_input = traces_torch[:, None, :] * pytorch_model.input_scale\n",
    "# channel_num = 0\n",
    "# start_frame = 0\n",
    "# test_input = model_input[\n",
    "#     channel_num : channel_num + 1,\n",
    "#     :,\n",
    "#     start_frame : start_frame + pytorch_model.sample_size,\n",
    "# ]\n",
    "\n",
    "# # Move the model to GPU if needed\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "pytorch_model.model.eval()  # Set model to evaluation mode\n",
    "# Convert all parameters to float32\n",
    "model = pytorch_model.model.float()  # This casts all parameters to torch.float32\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    torch.zeros(1, 1, pytorch_model.sample_size, dtype=torch.float32),\n",
    "    str(\"models/detect-mea.onnx\"),\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\", 2: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\", 2: \"sequence_length\"},\n",
    "    },\n",
    "    opset_version=12,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1b6a2b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch and ONNX outputs are concordant\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"models/detect-mea.onnx\")\n",
    "\n",
    "input_meta = ort_session.get_inputs()[0]\n",
    "input_name = input_meta.name\n",
    "onnx_input_shape = input_meta.shape\n",
    "\n",
    "test_input = traces_torch[:, None, :] * pytorch_model.input_scale * inference_scaling\n",
    "\n",
    "onnx_outputs = ort_session.run(\n",
    "    [\"output\"],\n",
    "    {\"input\": test_input.to(torch.float).numpy()},\n",
    ")\n",
    "\n",
    "print(\"Pytorch and ONNX outputs are concordant\")\n",
    "np.isclose(\n",
    "    pytorch_outputs.detach().numpy()[:, 0 : pytorch_model.num_output_locs],\n",
    "    onnx_outputs[0][:, 0 : pytorch_model.num_output_locs],\n",
    "    rtol=1e-2,\n",
    ").all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c92f41",
   "metadata": {},
   "source": [
    "## Find Spikes\n",
    "\n",
    "Plot the traces and fine spikes via threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the waveform as a time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "waveform_data = model_input.squeeze().numpy()  # Remove batch and channel dimensions\n",
    "time_points = np.arange(len(waveform_data))\n",
    "plt.plot(time_points, waveform_data)\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Waveform Time Series\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05677186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find spikes (values below -30) in model_input\n",
    "waveform_data = model_input.squeeze().numpy()\n",
    "\n",
    "# Handle both single and multi-channel cases\n",
    "if len(waveform_data.shape) == 2:\n",
    "    num_channels, sequence_length = waveform_data.shape\n",
    "\n",
    "    spike_locations = []\n",
    "    for channel in range(num_channels):\n",
    "        # Find sample indices where values go below -30\n",
    "        spike_samples = np.where(waveform_data[channel] < -30)[0]\n",
    "\n",
    "        if len(spike_samples) > 0:\n",
    "            print(\n",
    "                f\"Channel {channel}: {len(spike_samples)} spikes at samples {spike_samples}\"\n",
    "            )\n",
    "            # Store channel and sample pairs\n",
    "            for sample in spike_samples:\n",
    "                spike_locations.append(\n",
    "                    (channel, sample, waveform_data[channel, sample])\n",
    "                )\n",
    "        # else:\n",
    "        #     print(f\"Channel {channel}: No spikes detected\")\n",
    "\n",
    "    # Summary of all spikes\n",
    "    print(f\"\\nTotal spikes found: {len(spike_locations)}\")\n",
    "    if spike_locations:\n",
    "        print(\"(Channel, Sample, Value):\")\n",
    "        for channel, sample, value in spike_locations:\n",
    "            print(f\"  ({channel}, {sample}, {value:.2f})\")\n",
    "\n",
    "else:\n",
    "    # Single channel case\n",
    "    spike_samples = np.where(waveform_data < -30)[0]\n",
    "    print(f\"Single channel: {len(spike_samples)} spikes at samples {spike_samples}\")\n",
    "    if len(spike_samples) > 0:\n",
    "        for sample in spike_samples:\n",
    "            print(f\"  Sample {sample}: {waveform_data[sample]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fdb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_num = 281\n",
    "start_frame = 2725\n",
    "test_input = model_input[\n",
    "    channel_num : channel_num + 1,\n",
    "    :,\n",
    "    start_frame : start_frame + pytorch_model.sample_size,\n",
    "]\n",
    "\n",
    "# Plot the waveform as a time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "waveform_data = test_input.squeeze().numpy()  # Remove batch and channel dimensions\n",
    "time_points = np.arange(len(waveform_data))\n",
    "plt.plot(time_points, waveform_data)\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Waveform Time Series\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641a7d8",
   "metadata": {},
   "source": [
    "## PyTorch Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_outputs = pytorch_model.model(test_input).cpu()\n",
    "\n",
    "np.save(\"data/pytorch/model_outputs.npy\", pytorch_outputs.detach().numpy())\n",
    "pytorch_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd25ce1",
   "metadata": {},
   "source": [
    "# Run via ONNX Python Runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"models/detect-mea.onnx\")\n",
    "\n",
    "input_meta = ort_session.get_inputs()[0]\n",
    "input_name = input_meta.name\n",
    "onnx_input_shape = input_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_outs = ort_session.run([\"output\"], {\"input\": test_input.to(torch.float).numpy()})\n",
    "ort_outs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
