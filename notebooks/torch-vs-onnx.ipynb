{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce503323",
   "metadata": {},
   "source": [
    "# Torch vs. ONNX\n",
    "\n",
    "Verify that the spike detection model output of braindance vs. torch vs. onnx in python is concordant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8f40de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7293e7",
   "metadata": {},
   "source": [
    "## Load Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b0240c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a trained model\n",
    "from braindance.core.spikedetector.model import ModelSpikeSorter\n",
    "\n",
    "# detection_model = ModelSpikeSorter.load(\"checkpoints/spikedetector/mea\")\n",
    "\n",
    "with open(\"checkpoints/spikedetector/mea/init_dict.json\", \"r\") as f:\n",
    "    init_dict = json.load(f)\n",
    "pytorch_model = ModelSpikeSorter(**init_dict)\n",
    "state_dict = torch.load(\n",
    "    \"checkpoints/spikedetector/mea/state_dict.pt\", map_location=\"cpu\"\n",
    ")\n",
    "pytorch_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09447e",
   "metadata": {},
   "source": [
    "## Run via Braindance\n",
    "Populates data/inter if debug=False\n",
    "\n",
    "```\n",
    "Saving traces:\n",
    "100%|██████████| 1/1 [00:04<00:00,  4.46s/it]\n",
    "Running detection model:\n",
    "Compiling detection model for 942 elecs ...\n",
    "Cannot compile detection model with torch_tensorrt because cannot load torch_tensorrt. Skipping NVIDIA compilation\n",
    "Allocating disk space to save model traces and outputs ...\n",
    "Inference scaling: 0.3761194029850746\n",
    "Running model ...\n",
    "100%|██████████| 832/832 [09:36<00:00,  1.44it/s]\n",
    "Detecting sequences\n",
    "100%|██████████| 942/942 [00:04<00:00, 211.68it/s]\n",
    "Detected 10 preliminary propagation sequences\n",
    "Extracting sequences' detections, intervals, and amplitudes\n",
    "\n",
    "100%|██████████| 10/10 [00:02<00:00,  4.03it/s]\n",
    "8 clusters remain after filtering\n",
    "Reassigning spikes to preliminary propagation sequences\n",
    "Initializing ...\n",
    "Sorting recording\n",
    "100%|██████████| 1000/1000 [00:00<00:00, 3377.24it/s]\n",
    "Extracting sequences' detections, intervals, and amplitudes\n",
    "\n",
    "100%|██████████| 7/7 [00:02<00:00,  2.80it/s]\n",
    "7 clusters remain after filtering\n",
    "Merging preliminary propagation sequences - first round\n",
    "\n",
    "100%|██████████| 7/7 [00:02<00:00,  3.14it/s]\n",
    "7 sequences after first merging\n",
    "Merging preliminary propagation sequences - second round ...\n",
    "\n",
    "RT-Sort detected 7 sequences\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a958efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving traces:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running detection model:\n",
      "Compiling detection model for 942 elecs ...\n",
      "Cannot compile detection model with torch_tensorrt because cannot load torch_tensorrt. Skipping NVIDIA compilation\n",
      "Allocating disk space to save model traces and outputs ...\n",
      "Inference scaling: 0.3761194029850746\n",
      "Running model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 832/832 [09:57<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 942/942 [00:04<00:00, 213.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 10 preliminary propagation sequences\n",
      "Extracting sequences' detections, intervals, and amplitudes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 clusters remain after filtering\n",
      "Reassigning spikes to preliminary propagation sequences\n",
      "Initializing ...\n",
      "Sorting recording\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3306.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sequences' detections, intervals, and amplitudes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:02<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 clusters remain after filtering\n",
      "Merging preliminary propagation sequences - first round\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:02<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 sequences after first merging\n",
      "Merging preliminary propagation sequences - second round ...\n",
      "\n",
      "RT-Sort detected 7 sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from braindance.core.spikesorter.rt_sort import detect_sequences\n",
    "\n",
    "# Detect sequences in the first 5 minutes of a recording\n",
    "rt_sort = detect_sequences(\n",
    "    \"data/MEA_rec_patch_ground_truth_cell7.raw.h5\",\n",
    "    \"data/inter\",\n",
    "    pytorch_model,\n",
    "    recording_window_ms=(0, 5 * 1000),\n",
    "    device=\"cpu\",\n",
    "    verbose=True,\n",
    "    debug=True,\n",
    "    # num_processes=1,  # Uncomment for debugging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ac8d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load outputs for comparison below\n",
    "scaled_traces = np.load(\"data/inter/scaled_traces.npy\")\n",
    "# scaled_traces = np.load(\"data/inter/scaled_traces.npy\")[\n",
    "#     0:8, 0 : 10 * pytorch_model.sample_size\n",
    "# ]\n",
    "braindance_model_outputs = np.load(\"data/inter/model_outputs.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86793404",
   "metadata": {},
   "source": [
    "## Run via PyTorch\n",
    "Run using PyTorch with a simplified version of the code in braindance rtsort run_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2baa4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def run_detection_model(\n",
    "    scaled_traces,\n",
    "    model,\n",
    "    inference_scaling_numerator=12.6,\n",
    "    pre_median_frames=1000,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Simplified function to run a PyTorch detection model on scaled traces using windowed computation.\n",
    "\n",
    "    Parameters:\n",
    "        scaled_traces (np.ndarray): Input data array of shape (num_channels, recording_duration).\n",
    "        model: Pre-instantiated PyTorch model with attributes sample_size, num_output_locs, input_scale.\n",
    "        inference_scaling_numerator (float): Numerator for scaling factor calculation.\n",
    "        pre_median_frames (int): Number of frames for initial median calculation.\n",
    "        device (str): Device to run the model on (\"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Model outputs of shape (num_channels, processed_duration).\n",
    "    \"\"\"\n",
    "    # Convert input to torch tensor\n",
    "    scaled_traces = torch.tensor(scaled_traces, device=device, dtype=torch.float16)\n",
    "\n",
    "    # Get model parameters\n",
    "    sample_size = model.sample_size\n",
    "    num_output_locs = model.num_output_locs\n",
    "    input_scale = model.input_scale\n",
    "    num_chans, rec_duration = scaled_traces.shape\n",
    "\n",
    "    # Calculate inference scaling based on initial window\n",
    "    window = (\n",
    "        scaled_traces[:, :pre_median_frames].to(torch.float32).cpu()\n",
    "    )  # Cast to float32 and move to CPU\n",
    "    if window.dtype != torch.float32:\n",
    "        raise ValueError(\n",
    "            f\"Window tensor dtype is {window.dtype}, expected torch.float32\"\n",
    "        )\n",
    "    iqrs = torch.quantile(window, 0.75, dim=1) - torch.quantile(window, 0.25, dim=1)\n",
    "    median_iqr = torch.median(iqrs)\n",
    "    inference_scaling = (\n",
    "        inference_scaling_numerator / median_iqr if median_iqr != 0 else 1\n",
    "    )\n",
    "\n",
    "    # Define windows for processing\n",
    "    all_start_frames = list(range(0, rec_duration - sample_size + 1, num_output_locs))[\n",
    "        0:2\n",
    "    ]\n",
    "    output_duration = rec_duration - sample_size + 1\n",
    "    outputs_all = torch.zeros(\n",
    "        (num_chans, output_duration), device=device, dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Process each window\n",
    "    with torch.no_grad():\n",
    "        for start_frame in tqdm(all_start_frames):\n",
    "            # Extract window\n",
    "            traces_torch = scaled_traces[:, start_frame : start_frame + sample_size]\n",
    "\n",
    "            # Subtract median for baseline correction\n",
    "            traces_torch = (\n",
    "                traces_torch - torch.median(traces_torch, dim=1, keepdim=True).values\n",
    "            )\n",
    "\n",
    "            # Run model on window and store output\n",
    "            outputs = model.model(\n",
    "                traces_torch[:, None, :] * input_scale * inference_scaling\n",
    "            )\n",
    "            outputs_all[:, start_frame : start_frame + num_output_locs] = outputs\n",
    "\n",
    "    return outputs_all.cpu()\n",
    "\n",
    "\n",
    "pytorch_model_outputs = run_detection_model(\n",
    "    scaled_traces=scaled_traces, model=pytorch_model, device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632784c",
   "metadata": {},
   "source": [
    "## Compare Braindance to PyTorch Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32d29e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(\n",
    "    braindance_model_outputs[0:2, 0 : 2 * pytorch_model.num_output_locs],\n",
    "    pytorch_model_outputs.detach().numpy()[0:2, 0 : 2 * pytorch_model.num_output_locs],\n",
    "    rtol=1e-4,\n",
    ").all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447b7c1",
   "metadata": {},
   "source": [
    "## Run via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inference scaling factor based on the median IQR of the first 1000 frames\n",
    "import scipy.stats\n",
    "\n",
    "scaled_traces = np.load(\"data/inter/scaled_traces.npy\")\n",
    "\n",
    "# Defaults from rt_sort.py\n",
    "inference_scaling_numerator = 12.6\n",
    "pre_median_frames = 1000\n",
    "\n",
    "window = scaled_traces[:, :pre_median_frames]\n",
    "iqrs = scipy.stats.iqr(window, axis=1)\n",
    "median = np.median(iqrs)\n",
    "inference_scaling = inference_scaling_numerator / median\n",
    "print(\"inference_scaling:\", inference_scaling)\n",
    "\n",
    "# From output of detect_sequences call above should be 0.3761194029850746\n",
    "assert np.isclose(inference_scaling, 0.3761194029850746, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95696ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a single input frame to the model\n",
    "traces_torch = torch.tensor(\n",
    "    scaled_traces[:, 0 : pytorch_model.sample_size],\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "traces_torch -= torch.median(traces_torch, dim=1, keepdim=True).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef175ac4",
   "metadata": {},
   "source": [
    "## Compare PyTorch to Braindance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dda973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit comparison to the first num_output_locs locations as there is some overlap\n",
    "model_traces = np.load(\"data/inter/model_traces.npy\")\n",
    "print(\"Model inputs are concordant\")\n",
    "np.isclose(\n",
    "    traces_torch.numpy()[:, 0 : pytorch_model.num_output_locs],\n",
    "    model_traces[:, 0 : pytorch_model.num_output_locs],\n",
    "    rtol=1e-5,\n",
    ").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = pytorch_model.model(model_input[0:1, :, 0:100]).cpu()\n",
    "pytorch_outputs = pytorch_model.model(\n",
    "    traces_torch[:, None, :] * pytorch_model.input_scale * inference_scaling\n",
    ")\n",
    "model_outputs = np.load(\"data/inter/model_outputs.npy\")\n",
    "print(\"Model outputs are concordant\")\n",
    "np.isclose(\n",
    "    pytorch_outputs.detach().numpy()[:, 0 : pytorch_model.num_output_locs],\n",
    "    model_outputs[:, 0 : pytorch_model.num_output_locs],\n",
    "    rtol=1e-5,\n",
    ").all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ffffb",
   "metadata": {},
   "source": [
    "# Export to .onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# num_channels = init_dict[\"num_channels_in\"]\n",
    "# sample_size = init_dict[\"sample_size\"]\n",
    "\n",
    "# model_input = traces_torch[:, None, :] * pytorch_model.input_scale\n",
    "# channel_num = 0\n",
    "# start_frame = 0\n",
    "# test_input = model_input[\n",
    "#     channel_num : channel_num + 1,\n",
    "#     :,\n",
    "#     start_frame : start_frame + pytorch_model.sample_size,\n",
    "# ]\n",
    "\n",
    "# # Move the model to GPU if needed\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "pytorch_model.model.eval()  # Set model to evaluation mode\n",
    "# Convert all parameters to float32\n",
    "model = pytorch_model.model.float()  # This casts all parameters to torch.float32\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    torch.zeros(1, 1, pytorch_model.sample_size, dtype=torch.float32),\n",
    "    str(\"models/detect-mea.onnx\"),\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\", 2: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\", 2: \"sequence_length\"},\n",
    "    },\n",
    "    opset_version=12,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"models/detect-mea.onnx\")\n",
    "\n",
    "input_meta = ort_session.get_inputs()[0]\n",
    "input_name = input_meta.name\n",
    "onnx_input_shape = input_meta.shape\n",
    "\n",
    "test_input = traces_torch[:, None, :] * pytorch_model.input_scale * inference_scaling\n",
    "\n",
    "onnx_outputs = ort_session.run(\n",
    "    [\"output\"],\n",
    "    {\"input\": test_input.to(torch.float).numpy()},\n",
    ")\n",
    "\n",
    "print(\"Pytorch and ONNX outputs are concordant\")\n",
    "np.isclose(\n",
    "    pytorch_outputs.detach().numpy()[:, 0 : pytorch_model.num_output_locs],\n",
    "    onnx_outputs[0][:, 0 : pytorch_model.num_output_locs],\n",
    "    rtol=1e-2,\n",
    ").all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c92f41",
   "metadata": {},
   "source": [
    "## Find Spikes\n",
    "\n",
    "Plot the traces and fine spikes via threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the waveform as a time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "waveform_data = model_input.squeeze().numpy()  # Remove batch and channel dimensions\n",
    "time_points = np.arange(len(waveform_data))\n",
    "plt.plot(time_points, waveform_data)\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Waveform Time Series\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05677186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find spikes (values below -30) in model_input\n",
    "waveform_data = model_input.squeeze().numpy()\n",
    "\n",
    "# Handle both single and multi-channel cases\n",
    "if len(waveform_data.shape) == 2:\n",
    "    num_channels, sequence_length = waveform_data.shape\n",
    "\n",
    "    spike_locations = []\n",
    "    for channel in range(num_channels):\n",
    "        # Find sample indices where values go below -30\n",
    "        spike_samples = np.where(waveform_data[channel] < -30)[0]\n",
    "\n",
    "        if len(spike_samples) > 0:\n",
    "            print(\n",
    "                f\"Channel {channel}: {len(spike_samples)} spikes at samples {spike_samples}\"\n",
    "            )\n",
    "            # Store channel and sample pairs\n",
    "            for sample in spike_samples:\n",
    "                spike_locations.append(\n",
    "                    (channel, sample, waveform_data[channel, sample])\n",
    "                )\n",
    "        # else:\n",
    "        #     print(f\"Channel {channel}: No spikes detected\")\n",
    "\n",
    "    # Summary of all spikes\n",
    "    print(f\"\\nTotal spikes found: {len(spike_locations)}\")\n",
    "    if spike_locations:\n",
    "        print(\"(Channel, Sample, Value):\")\n",
    "        for channel, sample, value in spike_locations:\n",
    "            print(f\"  ({channel}, {sample}, {value:.2f})\")\n",
    "\n",
    "else:\n",
    "    # Single channel case\n",
    "    spike_samples = np.where(waveform_data < -30)[0]\n",
    "    print(f\"Single channel: {len(spike_samples)} spikes at samples {spike_samples}\")\n",
    "    if len(spike_samples) > 0:\n",
    "        for sample in spike_samples:\n",
    "            print(f\"  Sample {sample}: {waveform_data[sample]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fdb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_num = 281\n",
    "start_frame = 2725\n",
    "test_input = model_input[\n",
    "    channel_num : channel_num + 1,\n",
    "    :,\n",
    "    start_frame : start_frame + pytorch_model.sample_size,\n",
    "]\n",
    "\n",
    "# Plot the waveform as a time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "waveform_data = test_input.squeeze().numpy()  # Remove batch and channel dimensions\n",
    "time_points = np.arange(len(waveform_data))\n",
    "plt.plot(time_points, waveform_data)\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Waveform Time Series\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641a7d8",
   "metadata": {},
   "source": [
    "## PyTorch Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_outputs = pytorch_model.model(test_input).cpu()\n",
    "\n",
    "np.save(\"data/pytorch/model_outputs.npy\", pytorch_outputs.detach().numpy())\n",
    "pytorch_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd25ce1",
   "metadata": {},
   "source": [
    "# Run via ONNX Python Runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"models/detect-mea.onnx\")\n",
    "\n",
    "input_meta = ort_session.get_inputs()[0]\n",
    "input_name = input_meta.name\n",
    "onnx_input_shape = input_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_outs = ort_session.run([\"output\"], {\"input\": test_input.to(torch.float).numpy()})\n",
    "ort_outs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
