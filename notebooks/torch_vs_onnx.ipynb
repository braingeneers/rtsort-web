{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce503323",
   "metadata": {},
   "source": [
    "# Torch vs. ONNX\n",
    "\n",
    "Verify that the spike detection model output of braindance vs. torch vs. onnx in python is concordant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8f40de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7293e7",
   "metadata": {},
   "source": [
    "## Load Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b0240c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a trained model\n",
    "from braindance.core.spikedetector.model import ModelSpikeSorter\n",
    "\n",
    "# detection_model = ModelSpikeSorter.load(\"checkpoints/spikedetector/mea\")\n",
    "\n",
    "with open(\"checkpoints/spikedetector/mea/init_dict.json\", \"r\") as f:\n",
    "    init_dict = json.load(f)\n",
    "pytorch_model = ModelSpikeSorter(**init_dict)\n",
    "state_dict = torch.load(\n",
    "    \"checkpoints/spikedetector/mea/state_dict.pt\", map_location=\"cpu\"\n",
    ")\n",
    "pytorch_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09447e",
   "metadata": {},
   "source": [
    "## Run via Braindance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a958efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving traces:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running detection model:\n",
      "Compiling detection model for 16 elecs ...\n",
      "Cannot compile detection model with torch_tensorrt because cannot load torch_tensorrt. Skipping NVIDIA compilation\n",
      "Allocating disk space to save model traces and outputs ...\n",
      "Inference scaling: 0.35244755244755244\n",
      "Running model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 832/832 [00:12<00:00, 66.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 16/16 [00:02<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 0 preliminary propagation sequences\n",
      "Extracting sequences' detections, intervals, and amplitudes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 clusters remain after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from braindance.core.spikesorter.rt_sort import detect_sequences\n",
    "\n",
    "# Detect sequences in the first 5 minutes of a recording\n",
    "rt_sort = detect_sequences(\n",
    "    \"public/sample_maxwell_raw.h5\",\n",
    "    \"data/inter\",\n",
    "    pytorch_model,\n",
    "    recording_window_ms=(0, 5 * 1000),\n",
    "    device=\"cpu\",\n",
    "    verbose=True,\n",
    "    # debug=True,\n",
    "    # num_processes=1,  # Uncomment for debugging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac8d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load outputs for comparison below\n",
    "scaled_traces = np.load(\"data/inter/scaled_traces.npy\")\n",
    "braindance_model_outputs = np.load(\"data/inter/model_outputs.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86793404",
   "metadata": {},
   "source": [
    "## Run via PyTorch\n",
    "\n",
    "Run using PyTorch with a simplified version of the code in braindance rtsort run_detection_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2baa4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def run_detection_model(\n",
    "    scaled_traces,\n",
    "    model,\n",
    "    inference_scaling_numerator=12.6,\n",
    "    pre_median_frames=1000,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Simplified function to run a PyTorch detection model on scaled traces using windowed computation.\n",
    "\n",
    "    Parameters:\n",
    "        scaled_traces (np.ndarray): Input data array of shape (num_channels, recording_duration).\n",
    "        model: Pre-instantiated PyTorch model with attributes sample_size, num_output_locs, input_scale.\n",
    "        inference_scaling_numerator (float): Numerator for scaling factor calculation.\n",
    "        pre_median_frames (int): Number of frames for initial median calculation.\n",
    "        device (str): Device to run the model on (\"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Model outputs of shape (num_channels, processed_duration).\n",
    "    \"\"\"\n",
    "    # Convert input to torch tensor\n",
    "    scaled_traces = torch.tensor(scaled_traces, device=device, dtype=torch.float16)\n",
    "\n",
    "    # Get model parameters\n",
    "    sample_size = model.sample_size\n",
    "    num_output_locs = model.num_output_locs\n",
    "    input_scale = model.input_scale\n",
    "    num_chans, rec_duration = scaled_traces.shape\n",
    "\n",
    "    # Calculate inference scaling based on initial window\n",
    "    window = (\n",
    "        scaled_traces[:, :pre_median_frames].to(torch.float32).cpu()\n",
    "    )  # Cast to float32 and move to CPU\n",
    "    if window.dtype != torch.float32:\n",
    "        raise ValueError(\n",
    "            f\"Window tensor dtype is {window.dtype}, expected torch.float32\"\n",
    "        )\n",
    "    iqrs = torch.quantile(window, 0.75, dim=1) - torch.quantile(window, 0.25, dim=1)\n",
    "    median_iqr = torch.median(iqrs)\n",
    "    inference_scaling = (\n",
    "        inference_scaling_numerator / median_iqr if median_iqr != 0 else 1\n",
    "    )\n",
    "\n",
    "    # Define windows for processing\n",
    "    all_start_frames = list(range(0, rec_duration - sample_size + 1, num_output_locs))[\n",
    "        0:10\n",
    "    ]\n",
    "    output_duration = rec_duration - sample_size + 1\n",
    "    outputs_all = torch.zeros(\n",
    "        (num_chans, output_duration), device=device, dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Process each window\n",
    "    with torch.no_grad():\n",
    "        for start_frame in tqdm(all_start_frames):\n",
    "            # Extract window\n",
    "            traces_torch = scaled_traces[:, start_frame : start_frame + sample_size]\n",
    "\n",
    "            # Subtract median for baseline correction\n",
    "            traces_torch = (\n",
    "                traces_torch - torch.median(traces_torch, dim=1, keepdim=True).values\n",
    "            )\n",
    "\n",
    "            # Run model on window and store output\n",
    "            outputs = model.model(\n",
    "                traces_torch[:, None, :] * input_scale * inference_scaling\n",
    "            )\n",
    "            outputs_all[:, start_frame : start_frame + num_output_locs] = outputs\n",
    "\n",
    "    return outputs_all.cpu()\n",
    "\n",
    "\n",
    "pytorch_model_outputs = run_detection_model(\n",
    "    scaled_traces=scaled_traces, model=pytorch_model, device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632784c",
   "metadata": {},
   "source": [
    "## Compare Braindance to PyTorch Model Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e513300d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only did 10 frames above, so we can check the first 10 frames\n",
    "end = 10 * pytorch_model.num_output_locs\n",
    "np.isclose(\n",
    "    braindance_model_outputs[:, 0:end],\n",
    "    pytorch_model_outputs.detach().numpy()[:, 0:end],\n",
    "    rtol=1e-6,\n",
    ").all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ffffb",
   "metadata": {},
   "source": [
    "# Export to .onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7913d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "pytorch_model.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Convert all parameters to float32\n",
    "# model = pytorch_model.model.float()  # This casts all parameters to torch.float32\n",
    "\n",
    "torch.onnx.export(\n",
    "    pytorch_model.model,\n",
    "    torch.zeros(1, 1, pytorch_model.sample_size, dtype=torch.float16),\n",
    "    str(\"public/models/detect-mea.onnx\"),\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\", 2: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\", 2: \"sequence_length\"},\n",
    "    },\n",
    "    opset_version=12,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4853f8",
   "metadata": {},
   "source": [
    "## Run via PyTorch and ONNX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9af9511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import onnxruntime\n",
    "\n",
    "\n",
    "def run_detection_model_onnx(\n",
    "    scaled_traces,\n",
    "    model,\n",
    "    ort_session,\n",
    "    inference_scaling_numerator=12.6,\n",
    "    pre_median_frames=1000,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Simplified function to run a PyTorch detection model on scaled traces using windowed computation.\n",
    "\n",
    "    Parameters:\n",
    "        scaled_traces (np.ndarray): Input data array of shape (num_channels, recording_duration).\n",
    "        model: Pre-instantiated PyTorch model with attributes sample_size, num_output_locs, input_scale.\n",
    "        inference_scaling_numerator (float): Numerator for scaling factor calculation.\n",
    "        pre_median_frames (int): Number of frames for initial median calculation.\n",
    "        device (str): Device to run the model on (\"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Model outputs of shape (num_channels, processed_duration).\n",
    "    \"\"\"\n",
    "    # Convert input to torch tensor\n",
    "    scaled_traces = torch.tensor(scaled_traces, device=device, dtype=torch.float16)\n",
    "\n",
    "    # Get model parameters\n",
    "    sample_size = model.sample_size\n",
    "    num_output_locs = model.num_output_locs\n",
    "    input_scale = model.input_scale\n",
    "    num_chans, rec_duration = scaled_traces.shape\n",
    "\n",
    "    # Calculate inference scaling based on initial window\n",
    "    window = (\n",
    "        scaled_traces[:, :pre_median_frames].to(torch.float32).cpu()\n",
    "    )  # Cast to float32 and move to CPU\n",
    "    if window.dtype != torch.float32:\n",
    "        raise ValueError(\n",
    "            f\"Window tensor dtype is {window.dtype}, expected torch.float32\"\n",
    "        )\n",
    "    iqrs = torch.quantile(window, 0.75, dim=1) - torch.quantile(window, 0.25, dim=1)\n",
    "    median_iqr = torch.median(iqrs)\n",
    "    inference_scaling = (\n",
    "        inference_scaling_numerator / median_iqr if median_iqr != 0 else 1\n",
    "    )\n",
    "\n",
    "    # Define windows for processing\n",
    "    all_start_frames = list(range(0, rec_duration - sample_size + 1, num_output_locs))[\n",
    "        0:10\n",
    "    ]\n",
    "    output_duration = rec_duration - sample_size + 1\n",
    "    outputs_all = np.zeros((num_chans, output_duration), dtype=np.float16)\n",
    "\n",
    "    # Process each window\n",
    "    with torch.no_grad():\n",
    "        for start_frame in tqdm(all_start_frames):\n",
    "            # Extract window\n",
    "            traces_torch = scaled_traces[:, start_frame : start_frame + sample_size]\n",
    "\n",
    "            # Subtract median for baseline correction\n",
    "            traces_torch = (\n",
    "                traces_torch - torch.median(traces_torch, dim=1, keepdim=True).values\n",
    "            )\n",
    "\n",
    "            # Run model on window and store output\n",
    "            input_frame = traces_torch[:, None, :] * input_scale * inference_scaling\n",
    "\n",
    "            pytorch_outputs = model.model(input_frame)\n",
    "            onnx_outputs = ort_session.run(\n",
    "                [\"output\"],\n",
    "                {\"input\": input_frame.numpy()},\n",
    "            )[0]\n",
    "\n",
    "            match = np.isclose(\n",
    "                pytorch_outputs.detach().numpy(),\n",
    "                onnx_outputs,\n",
    "                rtol=1e-2,\n",
    "            ).all()\n",
    "            assert match, \"PyTorch and ONNX outputs do not match!\"\n",
    "\n",
    "            outputs_all[:, start_frame : start_frame + num_output_locs] = (\n",
    "                onnx_outputs.squeeze()\n",
    "            )\n",
    "\n",
    "    return outputs_all\n",
    "\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"public/models/detect-mea.onnx\")\n",
    "\n",
    "onnx_model_outputs = run_detection_model_onnx(\n",
    "    scaled_traces=scaled_traces,\n",
    "    model=pytorch_model,\n",
    "    ort_session=ort_session,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccbd60b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only did 20 frames above, so we can check the first 20 frames\n",
    "end = 10 * pytorch_model.num_output_locs\n",
    "np.isclose(\n",
    "    braindance_model_outputs[:, 0:end],\n",
    "    onnx_model_outputs[:, 0:end],\n",
    "    rtol=1e-2,\n",
    ").all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
